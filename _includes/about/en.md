
<span style="color: #4285F4;"><b>LLM Researcher | ByteDance Seed LLM | djhbarca[at]163.com</b></span>

üå±üíº: <span style="color:rgb(106, 0, 95);">We are currently recruiting interns for LLM research at Seed. If you are interested in contributing to this field, please reach out to me directly.</span>

Currently, I am an algorithm researcher in ByteDance Seed LLM team, and my main job responsibilities involve training algorithms and data mining.

### Education  
- Master of Computer Science @ <span style="color:rgb(106, 0, 95);"><b>Nanjing University</b></span> (2019‚Äì2022)  
- Bachelor of Computer Science @ <span style="color:rgb(106, 0, 95);"><b>Nanjing University</b></span> (2015‚Äì2019)  

### Research Interests

My work centers on machine learning and data mining methods with demonstrable theoretical foundations. Key research directions include:  
- üî¨ Training optimization algorithms and scalable training methodologies  
- üïµÔ∏è High-quality training data mining & curation  
- ‚öñÔ∏è Data distribution shift mitigation in deep learning  
- üß† Theoretical foundations of ultra-large-scale model training  
- ü§ñ Understanding paradigm and generalization of LLM/VLM  

### Latest Activity
- [üÜïLATEST Blog] Seed LLM&VLM Team. **[[Seed-1.6](https://seed.bytedance.com/en/seed1_6)]**
- [üÜïLATEST Paper] Haoran Zong, Xiao Zhang, Ruichen Li, <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Derun Zou, Wenzhong Li. <b>Convergence Guaranteed Federated Learning through Gradient Trajectory Smoothing with Triple-Objective Decomposition.</b>, ACM Transactions on Knowledge Discovery from Data, DOI: 10.1145/3743142, Jun 2025.


### Selected Publications [[Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=H6fOj2UAAAAJ)]
- Seed LLM&VLM Team. **[[Seed-1.6](https://seed.bytedance.com/en/seed1_6)]**
- Seed VLM&LLM Team. **Seed1.5-VL Technical Report**. arXiv:2505.07062. May. 2025.
- Seed LLM Team. **Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning**. arXiv:2504.13914. Apr. 2025.
- Haoran Zong, Xiao Zhang, Ruichen Li, <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Derun Zou, Wenzhong Li. <b>Convergence Guaranteed Federated Learning through Gradient Trajectory Smoothing with Triple-Objective Decomposition.</b>, ACM Transactions on Knowledge Discovery from Data, DOI: 10.1145/3743142, Jun 2025.
- <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Wenzhong Li, Derun Zou, Ruichen Li, Sanglu Lu, <b>Federated Learning with Data-Agnostic Distribution Fusion</b>, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023), Vancouver, Canada, Jun 18-22, 2023.
- <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Wenzhong Li, Sanglu Lu, <b>FedDNA: Federated Learning with Decoupled Normalization-Layer Aggregation for Non-IID Data</b>, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2021), Bilbao, Spain, Sep 13-17, 2021.
- <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Wenzhong Li, Xiao Zhang, Sanglu Lu, <b>Forecasting fine-grained city-scale cellular traffic with sparse crowdsourced measurements</b>, Computer Networks, 39(2461-2475), Volume 214, pp 1-14, Sep 4 2022.
- Wangxiang Ding, Wenzhong Li, Zhijie Zhang, Chen Wan, <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Sanglu Lu, <b>Time-varying Gaussian Markov Random Fields Learning for Multivariate Time Series Clustering</b>, IEEE Transactions on Knowledge and Data Engineering (TKDE), vol. 35, no. 11, Nov 2023.
- Derun Zou, Xusheng Liu, Lintan Sun, <span style="color: #4285F4;"><b>Jian-Hui Duan</b></span>, Ruichen Li, Yeting Xu, Wenzhong Li, Sanglu Lu, <b>FedMC: Federated Reinforcement Learning on the Edge with Meta-Critic Networks</b>, IEEE International Performance, Computing, and Communications Conference (IPCCC'22), Austin, Texas, USA, November 11-13, 2022.


### Contact  
- üìß Email: djhbarca[at]163.com
- üîó LinkedIn: [linkedin.com/in/jianhui-duan](https://www.linkedin.com/in/jianhui-duan-88824a125/) 